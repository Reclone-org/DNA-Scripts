{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reclone-org/DNA-Scripts/blob/main/RecloneSyntax_PrepareCDSforSynthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing a protein sequence for assembly with the Reclone Syntax\n",
        "\n",
        "# üß¨ Notebook purpose\n",
        "\n",
        "This notebook takes coding sequences (preferably **protein sequences**) and produces **synthesis-ready DNA parts** for Reclone-style assembly:\n",
        "\n",
        "* Optimizes codons for *E. coli* (DNA Chisel).\n",
        "* Enriches wobble bases in **codons 1‚Äì7** (relative to ATG) to favor A (and T for the first codon‚Äôs wobble).\n",
        "* Minimizes guanosines (G) via synonymous changes where possible (within codons 1‚Äì7).\n",
        "* Checks/avoids common restriction sites, long homopolymers, and enforces GC% windows (DNA Chisel).\n",
        "* Builds adaptered parts with **internal BsaI tags** (your chosen 5‚Ä≤/3‚Ä≤ syntax letters) and outer **BbsI** sites/overhangs for direct cloning of fragments into pOpenv3 (left `CGCT`, right `GGAG`) plus spacers. NB: we are currently [deliberating on storage vectors for Reclone](https://forum.reclone.org/t/reclone-storage-vectors-future-directions/1190/4) and may deprectae pOpen_v3.\n",
        "* Verifies adapter layout with **guardrails** so tags land on the **correct ends**.\n",
        "\n",
        "You‚Äôll get two FASTAs:\n",
        "\n",
        "1. **Enriched inserts** (no adapters)\n",
        "2. **Synthesis-ready parts** (with BsaI tags + spacers + BbsI adapters)\n",
        "\n",
        "(Optionally, a protein translation FASTA of the enriched inserts, if enabled.)\n",
        "\n",
        "---\n",
        "\n",
        "# üìÑ Input CSV format\n",
        "\n",
        "When prompted, upload a CSV with these columns (case-insensitive headers are accepted):\n",
        "\n",
        "* **Name** ‚Äî your part name (e.g., `mVirD2`, `sfGFP`).\n",
        "* **Seq** ‚Äî the **protein sequence** (single-letter AAs).\n",
        "\n",
        "  * If you include a leading **M**, that‚Äôs fine; the code treats the insert as starting **after** the start codon (ATG).\n",
        "  * If you accidentally provide **DNA** (only A/C/G/T), the notebook will print a warning and can still proceed.\n",
        "* **5' syntax position** ‚Äî the **5‚Ä≤ tag** (left tag).\n",
        "* **3' syntax position** ‚Äî the **3‚Ä≤ tag** (right tag).\n",
        "* **Enzyme** ‚Äî optional; defaults to **BsaI** for internal tags. (Other enzymes are allowed but aren‚Äôt auto-handled by the Reclone map.)\n",
        "\n",
        "### Allowed syntax tags\n",
        "\n",
        "A, B, C, D, E, F, N1, N2, N3, N4, N5\n",
        "\n",
        "\n",
        "> **Important:** The first tag (5‚Ä≤ column) is applied to the **left** end of the insert; the second tag (3‚Ä≤ column) to the **right** end.\n",
        "> Example: a row with `C` (5‚Ä≤) and `D` (3‚Ä≤) builds a **CD** part.\n",
        "\n",
        "### Example CSV\n",
        "\n",
        "```\n",
        "Name,Seq,5' syntax position,3' syntax position,Enzyme\n",
        "mVirD2,SEQPTRWQ...K,C,D,BsaI\n",
        "sfGFP,MSKGEELFTGVVP...,B,C,BsaI\n",
        "tagRFP,TGSHHHHHHGSG, N1, N5, BsaI\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üîß What the notebook does (pipeline)\n",
        "\n",
        "1. **Load CSV** and validate tags.\n",
        "\n",
        "   * Warns if `Seq` looks like plain DNA (only A/C/G/T).\n",
        "2. **(If Seq is protein)** Reverse-translate to DNA (bacterial table 11), favoring **low-G codons**.\n",
        "3. **DNA Chisel optimization:**\n",
        "\n",
        "   * `CodonOptimize(species=\"e_coli\")`\n",
        "   * Avoids: BsaI/SapI/BbsI/BsmBI/EcoRI/NotI/XbaI/SpeI/PstI/BtgZI/AarI/NheI/XhoI/BamHI/BglII/NruI\n",
        "   * Avoids homopolymers ‚â•6 (A/T/C/G)\n",
        "   * Enforces GC% in sliding windows (default 40‚Äì60% in 100 bp)\n",
        "4. **Early-codon enrichment (codons 1‚Äì7, after ATG):**\n",
        "\n",
        "   * Codon 1 wobble ‚àà {A,T}; codons 2‚Äì7 wobble = A.\n",
        "   * Then reduce G by synonymous changes (still within codons 1‚Äì7).\n",
        "   * Every edit preserves the amino-acid sequence and re-checks constraints.\n",
        "5. **Adapter assembly:**\n",
        "\n",
        "   * Internal **BsaI** tags from your 5‚Ä≤/3‚Ä≤ letters (position-aware scars as defined), plus linkers/spacers.\n",
        "   * Outer **BbsI** sites with fixed pOpenv3 overhangs: left `CGCT`, right `GGAG`.\n",
        "6. **Guardrails:**\n",
        "\n",
        "   * Verifies exact positions of BbsI sites/overhangs, BsaI sites, and your tag cores + scar bases on both ends.\n",
        "7. **Insert-only constraint check** inside the assembled construct (adapters are ignored in this check to avoid false failures).\n",
        "8. **Outputs** are written and a per-record **log** is printed with GC%, #edits, ŒîG, and guardrail status.\n",
        "\n",
        "---\n",
        "\n",
        "# üì¶ Outputs you‚Äôll see\n",
        "\n",
        "* `optimized_sequences_enriched_checked.fasta`\n",
        "  Enriched, constraint-checked **inserts only** (no adapters).\n",
        "  Description includes number of edits, AA/constraint status.\n",
        "\n",
        "* `synthesis_parts.fasta`\n",
        "  **Synthesis-ready parts** with your chosen 5‚Ä≤/3‚Ä≤ BsaI tags, spacers, and outer BbsI adapters.\n",
        "  IDs include the part name and the tag pair (e.g., `|CD`).\n",
        "\n",
        "* *(Optional)* `optimized_inserts_translation.fasta`\n",
        "  Protein translations of `ATG + insert` (helpful sanity check).\n",
        "\n",
        "---\n",
        "\n",
        "# ‚ö†Ô∏è Common messages & what they mean\n",
        "\n",
        "* **‚ÄúSeq contains only A/C/G/T (looks like DNA)‚Äù**\n",
        "  Your `Seq` appears to be DNA, not protein. If intentional, you can proceed; otherwise replace with the protein sequence.\n",
        "\n",
        "* **Guardrail FAIL**\n",
        "  A tag/site didn‚Äôt land where expected. The log shows which token failed (e.g., `Right_Bsa_core`). Check your tags and spacers.\n",
        "\n",
        "* **Constraint FAIL (after edits)**\n",
        "  DNA Chisel found a forbidden motif or GC/homopolymer issue after enrichment. The summary tells you where; adjust the input or relax constraints.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚ñ∂Ô∏è Quick start\n",
        "\n",
        "1. Prepare a CSV like the example above.\n",
        "2. Run the notebook cells in order.\n",
        "3. Watch the per-row logs for PASS/FAIL statuses.\n",
        "4. Download:\n",
        "\n",
        "   * **Inserts** ‚Üí `optimized_sequences_enriched_checked.fasta`\n",
        "   * **Parts**   ‚Üí `synthesis_parts.fasta`\n",
        "   * (Optional) **Protein translations** ‚Üí `optimized_inserts_translation.fasta`\n",
        "\n",
        "---\n",
        "\n",
        "# üìù Notes\n",
        "\n",
        "* The insert is assumed to start **immediately after** the start codon (**ATG**). If your protein `Seq` begins with **M**, that‚Äôs fine‚Äîthe pipeline accounts for it.\n",
        "* Overhang definitions are **position-aware**: the first tag uses its **5‚Ä≤** spec; the second tag uses its **3‚Ä≤** spec (including any scar bases).\n",
        "* Outer BbsI overhangs for pOpenv3 are fixed: **left `CGCT`**, **right `GGAG`**.\n",
        "\n",
        "If you want different scar defaults (e.g., serine instead of glycine for specific letters), edit the **5‚Ä≤/3‚Ä≤ overhang map** near the top of the notebook.\n"
      ],
      "metadata": {
        "id": "q-rABgFs_koU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "gkjU8KuKaPdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJGCCyLWN0Tp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65ca9c7-7e15-469c-9008-d55489c1f793",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package 'swig' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig3.0 is already the newest version (3.0.12-2.2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "ln: failed to create hard link '/usr/bin/swig': File exists\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libglpk-dev is already the newest version (5.0-1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.85)\n",
            "Requirement already satisfied: dnachisel in /usr/local/lib/python3.12/dist-packages (3.2.16)\n",
            "Requirement already satisfied: pydna in /usr/local/lib/python3.12/dist-packages (5.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: Bio in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.3.3)\n",
            "Requirement already satisfied: proglog in /usr/local/lib/python3.12/dist-packages (from dnachisel) (0.1.12)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.12/dist-packages (from dnachisel) (0.6.2)\n",
            "Requirement already satisfied: flametree in /usr/local/lib/python3.12/dist-packages (from dnachisel) (0.2.1)\n",
            "Requirement already satisfied: python_codon_tables in /usr/local/lib/python3.12/dist-packages (from dnachisel) (0.1.18)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.12/dist-packages (from pydna) (1.4.4)\n",
            "Requirement already satisfied: networkx>=2.8.8 in /usr/local/lib/python3.12/dist-packages (from pydna) (3.5)\n",
            "Requirement already satisfied: prettytable>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from pydna) (3.16.0)\n",
            "Requirement already satisfied: pydivsufsort>=0.0.14 in /usr/local/lib/python3.12/dist-packages (from pydna) (0.0.18)\n",
            "Requirement already satisfied: pyfiglet==0.8.post1 in /usr/local/lib/python3.12/dist-packages (from pydna) (0.8.post1)\n",
            "Requirement already satisfied: regex<2025.0.0,>=2024.11.6 in /usr/local/lib/python3.12/dist-packages (from pydna) (2024.11.6)\n",
            "Requirement already satisfied: seguid>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from pydna) (0.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: gprofiler-official in /usr/local/lib/python3.12/dist-packages (from Bio) (1.0.0)\n",
            "Requirement already satisfied: mygene in /usr/local/lib/python3.12/dist-packages (from Bio) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from Bio) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from Bio) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from Bio) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from Bio) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable>=3.5.0->pydna) (0.2.13)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from pydivsufsort>=0.0.14->pydna) (0.45.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.12/dist-packages (from mygene->Bio) (0.4.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch->Bio) (4.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (2025.8.3)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from biothings-client>=0.2.6->mygene->Bio) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get remove -y swig\n",
        "!apt-get install -y swig3.0\n",
        "!ln /usr/bin/swig3.0 /usr/bin/swig\n",
        "\n",
        "# Install necessary packages with verbose output\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential libglpk-dev zlib1g-dev\n",
        "# Added --upgrade --force-reinstall for biopython\n",
        "!pip install biopython dnachisel pydna matplotlib Bio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import relevant libraries"
      ],
      "metadata": {
        "id": "RbCdQp9paFWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= Imports =========================\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "from Bio.Data import CodonTable\n",
        "from dnachisel import (\n",
        "    DnaOptimizationProblem, AvoidPattern, EnforceGCContent, EnforceTranslation,\n",
        "    CodonOptimize, HomopolymerPattern\n",
        ")\n"
      ],
      "metadata": {
        "id": "L3sCFs78OS4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up adapter sites for Type IIS Assembly\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EYA2Zs7bZfb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========================= Adapter constants (fixed for pOpenv3) =========================\n",
        "BBSI_SITE_FW = \"GAAGAC\"\n",
        "BBSI_SITE_RV = \"GTCTTC\"\n",
        "BBSI_LEFT_OVERHANG  = \"CGCT\"\n",
        "BBSI_RIGHT_OVERHANG = \"GGAG\"\n",
        "BSAI_SITE_FW = \"GGTCTC\"\n",
        "BSAI_SITE_RV = \"GAGACC\"\n",
        "\n",
        "DEFAULT_FLANK5 = \"tgatc\"\n",
        "DEFAULT_FLANK3 = \"tacgt\"\n",
        "DEFAULT_BBSI_NN = \"at\"\n",
        "DEFAULT_BSAI_N  = \"a\"\n",
        "DEFAULT_SPACER_BETWEEN_SITES = \"atgct\"\n",
        "DEFAULT_SPACER_RIGHT         = \"tgcat\"\n",
        "\n",
        "\n",
        "# ========================= Reclone overhangs (position-aware) =========================\n",
        "# Canonical 4-nt overhang for each letter\n",
        "RECLONE_CANON: Dict[str, str] = {\n",
        "    \"A\":  \"GGAG\",\n",
        "    \"B\":  \"TACT\",\n",
        "    \"N1\": \"CCAT\",\n",
        "    \"N2\": \"GTCA\",\n",
        "    \"N3\": \"TCCA\",\n",
        "    \"C\":  \"AATG\",\n",
        "    \"D\":  \"AGGT\",\n",
        "    \"N4\": \"TTCG\",\n",
        "    \"N5\": \"CGGC\",\n",
        "    \"E\":  \"GCTT\",\n",
        "    \"F\":  \"CGCT\",\n",
        "    # (Optional back-compat) generic N alias; remove if not used:\n",
        "    \"N\":  \"TCCA\",\n",
        "}\n",
        "\n",
        "# Position-specific specs (5‚Ä≤ and 3‚Ä≤) including any extra scar bases.\n",
        "# Lowercase = extra bases outside the canonical 4-nt overhang.\n",
        "# List reflects your corrected table (defaults to gly/met where shown).\n",
        "RECLONE_END_SPECS: Dict[str, Dict[str, str]] = {\n",
        "    \"A\":  {\"5\": \"GGAG\",    \"3\": \"GGAG\"},\n",
        "    \"B\":  {\"5\": \"TACT\",    \"3\": \"TACT\"},\n",
        "    \"N1\": {\"5\": \"CCATg\",   \"3\": \"tCCAT\"},\n",
        "    \"N2\": {\"5\": \"GTCA\",    \"3\": \"ggGTCA\"},\n",
        "    \"N3\": {\"5\": \"TCCAtg\",  \"3\": \"TCCA\"},\n",
        "    \"C\":  {\"5\": \"AATG\",    \"3\": \"ggAATG\"},\n",
        "    \"D\":  {\"5\": \"AGGT\",    \"3\": \"ggAGGT\"},\n",
        "    \"N4\": {\"5\": \"TTCG\",  \"3\": \"ggTTCG\"},\n",
        "    \"N5\": {\"5\": \"CGGC\",    \"3\": \"ggCGGC\"},\n",
        "    \"E\":  {\"5\": \"GCTT\",    \"3\": \"GCTT\"},\n",
        "    \"F\":  {\"5\": \"CGCT\",    \"3\": \"CGCT\"},\n",
        "    # (Optional back-compat) generic N uses plain cores both ends:\n",
        "    \"N\":  {\"5\": \"TCCA\",    \"3\": \"TCCA\"},\n",
        "}\n",
        "\n",
        "def _parse_end_spec(spec: str, canonical: str) -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Parse a position-specific spec into (prefix, overhang, suffix).\n",
        "    - 'spec' may include lowercase extras and spaces (e.g., \"ggG TCA\" -> prefix=\"gg\", overhang=\"GTCA\", suffix=\"\").\n",
        "    - 'canonical' must match the uppercase core extracted from spec.\n",
        "    Returns uppercase strings.\n",
        "    \"\"\"\n",
        "    s = spec.replace(\" \", \"\")\n",
        "    # leading lowercase prefix\n",
        "    i = 0\n",
        "    while i < len(s) and s[i].islower():\n",
        "        i += 1\n",
        "    # trailing lowercase suffix\n",
        "    j = len(s) - 1\n",
        "    while j >= 0 and s[j].islower():\n",
        "        j -= 1\n",
        "    prefix = s[:i]\n",
        "    core   = s[i:j+1] if j >= i else \"\"\n",
        "    suffix = s[j+1:] if j+1 < len(s) else \"\"\n",
        "    if core.upper() != canonical:\n",
        "        raise ValueError(f\"Spec '{spec}' core '{core.upper()}' != canonical overhang '{canonical}'\")\n",
        "    return prefix.upper(), canonical, suffix.upper()\n",
        "\n",
        "def resolve_bsa_end(letter: str, side: str) -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Resolve a BsaI overhang 'letter' at 5‚Ä≤ or 3‚Ä≤ into (prefix, overhang, suffix), all uppercase.\n",
        "    'side' must be '5' or '3'.\n",
        "    \"\"\"\n",
        "    L = letter.strip().upper()\n",
        "    if L not in RECLONE_CANON:\n",
        "        raise ValueError(f\"Unknown overhang letter '{letter}'. Valid: {', '.join(sorted(RECLONE_CANON))}\")\n",
        "    if L not in RECLONE_END_SPECS or side not in RECLONE_END_SPECS[L]:\n",
        "        raise ValueError(f\"No end spec for {L} at {side}‚Ä≤.\")\n",
        "    return _parse_end_spec(RECLONE_END_SPECS[L][side], RECLONE_CANON[L])"
      ],
      "metadata": {
        "id": "f6KGbDYnZd-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up domesticatation and codon optimisation constraints for E.coli"
      ],
      "metadata": {
        "id": "NQa9n667QM0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= Global config =========================\n",
        "def build_constraints():\n",
        "    return [\n",
        "        AvoidPattern(\"BsaI_site\", strand='both'),\n",
        "        AvoidPattern(\"SapI_site\", strand='both'),\n",
        "        AvoidPattern(\"BbsI_site\", strand='both'),\n",
        "        AvoidPattern(\"BsmBI_site\", strand='both'),\n",
        "        AvoidPattern(\"EcoRI_site\", strand='both'),\n",
        "        AvoidPattern(\"NotI_site\", strand='both'),\n",
        "        AvoidPattern(\"XbaI_site\", strand='both'),\n",
        "        AvoidPattern(\"SpeI_site\", strand='both'),\n",
        "        AvoidPattern(\"PstI_site\", strand='both'),\n",
        "        AvoidPattern(\"BtgZI_site\", strand='both'),\n",
        "        AvoidPattern(\"AarI_site\", strand='both'),\n",
        "        AvoidPattern(\"NheI_site\", strand='both'),\n",
        "        AvoidPattern(\"XhoI_site\", strand='both'),\n",
        "        AvoidPattern(\"BamHI_site\", strand='both'),\n",
        "        AvoidPattern(\"BglII_site\", strand='both'),\n",
        "        AvoidPattern(\"NruI_site\", strand='both'),\n",
        "        AvoidPattern(HomopolymerPattern(\"A\", 6)),\n",
        "        AvoidPattern(HomopolymerPattern(\"T\", 6)),\n",
        "        AvoidPattern(HomopolymerPattern(\"C\", 6)),\n",
        "        AvoidPattern(HomopolymerPattern(\"G\", 6)),\n",
        "        EnforceGCContent(mini=0.4, maxi=0.6, window=100),\n",
        "        EnforceTranslation(genetic_table='Bacterial'),\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "sUgzfeo_ZIuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import a list of protein sequences of interest"
      ],
      "metadata": {
        "id": "5oBU-evvP5SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CSV loader (two separate tag columns) ---\n",
        "import csv\n",
        "from typing import List, Dict, Any\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "\n",
        "# Allowed tag tokens (5‚Ä≤/3‚Ä≤ syntax letters)\n",
        "VALID_TAGS = {\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"N1\",\"N2\",\"N3\",\"N4\",\"N5\",\"N\"}  # 'N' optional legacy\n",
        "\n",
        "def _ci_lookup(row: Dict[str, Any], *candidates: str) -> str | None:\n",
        "    \"\"\"Case-insensitive lookup of a field name from a CSV DictReader row.\"\"\"\n",
        "    lower_map = {k.lower(): k for k in row.keys()}\n",
        "    for c in candidates:\n",
        "        k = lower_map.get(c.lower())\n",
        "        if k is not None:\n",
        "            val = row.get(k)\n",
        "            if val is not None and str(val).strip() != \"\":\n",
        "                return str(val).strip()\n",
        "    return None\n",
        "\n",
        "def _looks_like_plain_dna(seq: str) -> bool:\n",
        "    s = (seq or \"\").upper().replace(\" \", \"\")\n",
        "    return len(s) > 0 and set(s) <= set(\"ACGT\")\n",
        "\n",
        "def load_parts_csv_two_cols(path: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Expected headers (case-insensitive):\n",
        "      - Name\n",
        "      - Seq\n",
        "      - 5' syntax position   (left / 5‚Ä≤ tag)\n",
        "      - 3' syntax position   (right / 3‚Ä≤ tag)\n",
        "      - Enzyme               (optional; defaults to BsaI)\n",
        "\n",
        "    Returns: list of dicts -> {name, seq, left_tag, right_tag, enzyme}\n",
        "    \"\"\"\n",
        "    rows: List[Dict[str, str]] = []\n",
        "    tag_counts = Counter()\n",
        "    with open(path, newline=\"\") as fh:\n",
        "        rdr = csv.DictReader(fh)\n",
        "        if rdr.fieldnames is None:\n",
        "            raise ValueError(\"CSV appears to have no header row.\")\n",
        "\n",
        "        for idx, row in enumerate(rdr, start=1):\n",
        "            name = _ci_lookup(row, \"Name\") or f\"row_{idx}\"\n",
        "            seq  = _ci_lookup(row, \"Seq\", \"Sequence\") or \"\"\n",
        "            left = (_ci_lookup(row, \"5' syntax position\", \"5prime\", \"left\", \"left_tag\") or \"\").upper()\n",
        "            right = (_ci_lookup(row, \"3' syntax position\", \"3prime\", \"right\", \"right_tag\") or \"\").upper()\n",
        "            enzyme = (_ci_lookup(row, \"Enzyme\") or \"BsaI\").upper()\n",
        "\n",
        "            # Validate tags\n",
        "            if not left or not right:\n",
        "                raise ValueError(f\"[{name}] Missing 5‚Ä≤/3‚Ä≤ syntax tags.\")\n",
        "            if left not in VALID_TAGS:\n",
        "                raise ValueError(f\"[{name}] Invalid 5‚Ä≤ tag '{left}'. Allowed: {', '.join(sorted(VALID_TAGS))}\")\n",
        "            if right not in VALID_TAGS:\n",
        "                raise ValueError(f\"[{name}] Invalid 3‚Ä≤ tag '{right}'. Allowed: {', '.join(sorted(VALID_TAGS))}\")\n",
        "\n",
        "            tag_counts[left] += 1\n",
        "            tag_counts[right] += 1\n",
        "\n",
        "            # Heads-up if Seq looks like plain DNA (common input mixup)\n",
        "            if _looks_like_plain_dna(seq):\n",
        "                print(f\"‚ö†Ô∏è  {name}: Seq contains only A/C/G/T (looks like DNA). \"\n",
        "                      f\"If you intended a protein sequence here, double-check your CSV. \"\n",
        "                      f\"(Proceeding as-is; comment out this check if intentional.)\")\n",
        "\n",
        "            # Gentle reminder for non-BsaI rows (so you can route them differently downstream)\n",
        "            if enzyme != \"BSAI\":\n",
        "                print(f\"‚ÑπÔ∏è  {name}: Enzyme='{enzyme}'. Ensure downstream handling matches this enzyme.\")\n",
        "\n",
        "            rows.append({\n",
        "                \"name\": name,\n",
        "                \"seq\": seq,\n",
        "                \"left_tag\": left,    # 5‚Ä≤ tag (left)\n",
        "                \"right_tag\": right,  # 3‚Ä≤ tag (right)\n",
        "                \"enzyme\": enzyme,\n",
        "            })\n",
        "\n",
        "    print(f\"üì• Loaded {len(rows)} row(s) from {path}. Tag usage: {dict(tag_counts)}\")\n",
        "    print(\"‚û°Ô∏è  Convention: 5‚Ä≤ tag applies to the LEFT end; 3‚Ä≤ tag to the RIGHT end.\")\n",
        "    return rows\n",
        "\n",
        "# --- Upload + load (call AFTER the function is defined) ---\n",
        "uploaded = files.upload()                 # pick your CSV\n",
        "csv_path = list(uploaded.keys())[0]       # handles \"name (2).csv\" etc.\n",
        "parts = load_parts_csv_two_cols(csv_path)\n",
        "\n",
        "print(f\"\\nFirst row preview:\\n{parts[0] if parts else 'No rows'}\")\n"
      ],
      "metadata": {
        "id": "ZwmsbUBsOVaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "39e204dc-a077-4da2-bef0-6337e5330036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-086a3242-c508-4630-b3b0-92d808baae75\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-086a3242-c508-4630-b3b0-92d808baae75\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-209618230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# --- Upload + load (call AFTER the function is defined) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# pick your CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m       \u001b[0;31m# handles \"name (2).csv\" etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_parts_csv_two_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Reverse translate into DNA\n"
      ],
      "metadata": {
        "id": "iefaZCi2QISg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Re)init your output container\n",
        "dna_records = []\n",
        "\n",
        "# Ensure necessary BioPython imports are available\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio.Data import CodonTable\n",
        "\n",
        "# Get the bacterial codon table\n",
        "BACTERIAL_TABLE = CodonTable.unambiguous_dna_by_id[11]\n",
        "\n",
        "# Create a mapping from amino acid to a list of possible codons\n",
        "AA_TO_CODONS_LIST: Dict[str, List[str]] = {}\n",
        "for codon, aa in BACTERIAL_TABLE.forward_table.items():\n",
        "    AA_TO_CODONS_LIST.setdefault(aa, []).append(codon)\n",
        "\n",
        "# Iterate over the CSV rows returned by `load_parts_csv_two_cols`\n",
        "# Each row looks like: {\"name\", \"seq\", \"left_tag\", \"right_tag\", \"enzyme\"}\n",
        "for row in parts:\n",
        "    name = row[\"name\"]\n",
        "    protein_seq_str = row[\"seq\"] # The input protein sequence string\n",
        "    left_tag = row.get(\"left_tag\", \"\")\n",
        "    right_tag = row.get(\"right_tag\", \"\")\n",
        "    enzyme = row.get(\"enzyme\", \"\")\n",
        "\n",
        "    print(f\"\\nProcessing {name}...\")\n",
        "\n",
        "    # Skip if protein sequence is empty\n",
        "    if not protein_seq_str:\n",
        "        print(f\"  üö´ Skipping {name} due to empty protein sequence.\")\n",
        "        continue\n",
        "\n",
        "    initial_dna_seq = \"\" # Initialize DNA sequence for this record\n",
        "    translation_successful = True\n",
        "\n",
        "    try:\n",
        "        # Perform basic reverse translation manually using the codon table\n",
        "        dna_sequence_parts = []\n",
        "        for aa in protein_seq_str:\n",
        "            # Get possible codons for the amino acid\n",
        "            codons = AA_TO_CODONS_LIST.get(aa.upper()) # Use upper() for safety\n",
        "\n",
        "            if not codons:\n",
        "                # Handle amino acids not found in the codon table (e.g., 'X', 'Z')\n",
        "                print(f\"    ‚ö†Ô∏è Warning: Amino acid '{aa}' not found in bacterial codon table for {name}. Skipping codon.\")\n",
        "                translation_successful = False\n",
        "                break # Stop translation for this sequence\n",
        "\n",
        "            # For basic reverse translation, just pick the first codon from the list\n",
        "            # Note: This is a simplistic approach. BioPython's back_translate might use\n",
        "            # codon usage frequencies, but this avoids the AttributeError.\n",
        "            chosen_codon = codons[0]\n",
        "            dna_sequence_parts.append(chosen_codon)\n",
        "\n",
        "        if translation_successful and dna_sequence_parts:\n",
        "            initial_dna_seq = \"\".join(dna_sequence_parts)\n",
        "            print(f\"  ‚Ä¢ Basic reverse translation done manually: len={len(initial_dna_seq)} bp\")\n",
        "            # We cannot calculate GC% here without the _gc_pct function being available in this cell's scope\n",
        "            # print(f\"    GC={_gc_pct(initial_dna_seq):.1f}%\") # Assumes _gc_pct is defined elsewhere\n",
        "\n",
        "            # Removed the problematic translation verification step\n",
        "\n",
        "        elif not translation_successful:\n",
        "             print(f\"  üö´ Skipping {name} due to untranslatable amino acids.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error during manual reverse translation for {name}: {e}\")\n",
        "        initial_dna_seq = \"\" # Set to empty if translation fails\n",
        "        continue # Skip this record if translation failed\n",
        "\n",
        "\n",
        "    # Build a SeqRecord for the initial DNA sequence\n",
        "    if initial_dna_seq: # Only create record if we have a sequence\n",
        "        rec = SeqRecord(\n",
        "            Seq(initial_dna_seq),\n",
        "            id=name,\n",
        "            name=name,\n",
        "            description=f\"Initial DNA | {left_tag}|{right_tag}|{enzyme}\"\n",
        "        )\n",
        "        dna_records.append(rec)\n",
        "        print(f\"  ‚úÖ Added {name} ({len(initial_dna_seq)} bp) to dna_records.\")\n",
        "    else:\n",
        "        print(f\"  üö´ Skipping {name} due to empty sequence after processing or translation failure.\")\n",
        "\n",
        "\n",
        "# Quick inspect of the populated dna_records list\n",
        "print(\"\\n--- Quick inspect of dna_records ---\")\n",
        "if dna_records:\n",
        "    for r in dna_records:\n",
        "        print(f\"ID: {r.id}, Length: {len(r.seq)} bp\")\n",
        "else:\n",
        "    print(\"dna_records list is empty.\")"
      ],
      "metadata": {
        "id": "zzfAkzgfOev5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00f220d-dd59-47c1-bdbe-2fda9abdc1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing mVirD2...\n",
            "  ‚Ä¢ Basic reverse translation done manually: len=27 bp\n",
            "  ‚úÖ Added mVirD2 (27 bp) to dna_records.\n",
            "\n",
            "Processing sfGFP...\n",
            "  ‚Ä¢ Basic reverse translation done manually: len=39 bp\n",
            "  ‚úÖ Added sfGFP (39 bp) to dna_records.\n",
            "\n",
            "Processing tagRFP...\n",
            "  ‚Ä¢ Basic reverse translation done manually: len=36 bp\n",
            "  ‚úÖ Added tagRFP (36 bp) to dna_records.\n",
            "\n",
            "--- Quick inspect of dna_records ---\n",
            "ID: mVirD2, Length: 27 bp\n",
            "ID: sfGFP, Length: 39 bp\n",
            "ID: tagRFP, Length: 36 bp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ========================= Codon utilities =========================\n",
        "# BACTERIAL_TABLE = CodonTable.unambiguous_dna_by_id[11]\n",
        "# CODON_TO_AA = BACTERIAL_TABLE.forward_table.copy()\n",
        "# STOP_CODONS = set(BACTERIAL_TABLE.stop_codons)\n",
        "# AA_TO_CODONS: Dict[str, List[str]] = {}\n",
        "# for codon, aa in CODON_TO_AA.items():\n",
        "#     AA_TO_CODONS.setdefault(aa, []).append(codon)\n",
        "\n",
        "# # Sequences start AFTER ATG; edit codons 1..7 of provided insert\n",
        "# WOBBLE_WINDOW = (1, 7)\n",
        "# WOBBLE_FIRST_ALLOWED = {\"A\", \"T\"}  # codon 1 wobble\n",
        "# WOBBLE_OTHERS_ALLOWED = {\"A\"}      # codons 2..7 wobble\n",
        "# LOW_G_WINDOW = WOBBLE_WINDOW\n",
        "\n",
        "# def _hamming(a: str, b: str) -> int:\n",
        "#     return sum(x != y for x, y in zip(a, b))\n",
        "\n",
        "# def _ordered_wobble_candidates(aa: str, codon: str, allowed: Set[str]) -> List[str]:\n",
        "#     if aa not in AA_TO_CODONS:\n",
        "#         return []\n",
        "#     first2 = codon[:2]\n",
        "#     cands = []\n",
        "#     for wob in sorted(allowed):\n",
        "#         cand = (first2 + wob).upper()\n",
        "#         if cand in AA_TO_CODONS[aa]: # Corrected typo here\n",
        "#             cands.append(cand)\n",
        "#     others = [c for c in AA_TO_CODONS[aa] if c[-1] in allowed and c not in cands]\n",
        "#     others.sort(key=lambda c: (_hamming(c, codon), c))\n",
        "#     cands.extend(others)\n",
        "#     return cands\n",
        "\n",
        "# def _ordered_lowG_candidates(aa: str, codon: str, allowed: Optional[Set[str]]) -> List[str]:\n",
        "#     if aa not in AA_TO_CODONS:\n",
        "#         return []\n",
        "#     pool = AA_TO_CODONS[aa]\n",
        "#     if allowed is not None:\n",
        "#         pool = [c for c in pool if c[-1] in allowed]\n",
        "#     pool = [c for c in pool if c.count(\"G\") < codon.count(\"G\")]\n",
        "#     first2 = codon[:2]\n",
        "#     pool.sort(key=lambda c: (c.count(\"G\"), 0 if c[:2] == first2 else 1, _hamming(c, codon), c))\n",
        "#     return pool\n",
        "\n",
        "# def _passes_constraints_whole(seq_str: str) -> bool:\n",
        "#     p = DnaOptimizationProblem(sequence=seq_str, constraints=build_constraints())\n",
        "#     return p.all_constraints_pass()\n",
        "\n",
        "# def _aa(seq: str) -> str:\n",
        "#     return str(Seq(seq).translate(table=11))\n",
        "\n",
        "# def wobble_and_reduce(seq: str) -> Tuple[str, List[Tuple[int, str, str, str]]]:\n",
        "#     s = str(seq).upper()\n",
        "#     edits: List[Tuple[int, str, str, str]] = []\n",
        "#     if len(s) < 3 or len(s) % 3 != 0:\n",
        "#         return s, edits\n",
        "\n",
        "#     baseline_aa = _aa(s)  # cache and update after each accepted edit\n",
        "#     n_codons = len(s) // 3\n",
        "\n",
        "#     # Pass 1: wobble\n",
        "#     w_start, w_end = WOBBLE_WINDOW\n",
        "#     w_end = min(w_end, n_codons)\n",
        "#     if w_start <= w_end:\n",
        "#         for idx in range(w_start, w_end + 1):\n",
        "#             pos = (idx - 1) * 3\n",
        "#             codon = s[pos:pos+3]\n",
        "#             if codon in STOP_CODONS:\n",
        "#                 break\n",
        "#             aa = CODON_TO_AA.get(codon)\n",
        "#             if not aa:\n",
        "#                 continue\n",
        "#             allowed = WOBBLE_FIRST_ALLOWED if idx == w_start else WOBBLE_OTHERS_ALLOWED\n",
        "#             if codon[-1] in allowed:\n",
        "#                 continue\n",
        "#             for repl in _ordered_wobble_candidates(aa, codon, allowed):\n",
        "#                 tentative = s[:pos] + repl + s[pos+3:]\n",
        "#                 if _passes_constraints_whole(tentative) and _aa(tentative) == baseline_aa:\n",
        "#                     s = tentative\n",
        "#                     baseline_aa = _aa(s)\n",
        "#                     edits.append((idx, codon, repl, \"wobble\"))\n",
        "#                     break\n",
        "\n",
        "#     # Pass 2: low-G within window\n",
        "#     lg_start, lg_end = LOW_G_WINDOW\n",
        "#     lg_end = min(lg_end, n_codons)\n",
        "#     if lg_start <= lg_end:\n",
        "#         for idx in range(lg_start, lg_end + 1):\n",
        "#             pos = (idx - 1) * 3\n",
        "#             codon = s[pos:pos+3]\n",
        "#             if codon in STOP_CODONS:\n",
        "#                 break\n",
        "#             aa = CODON_TO_AA.get(codon)\n",
        "#             if not aa:\n",
        "#                 continue\n",
        "#             allowed = WOBBLE_FIRST_ALLOWED if idx == w_start else WOBBLE_OTHERS_ALLOWED\n",
        "#             for repl in _ordered_lowG_candidates(aa, codon, allowed):\n",
        "#                 tentative = s[:pos] + repl + s[pos+3:]\n",
        "#                 if _passes_constraints_whole(tentative) and _aa(tentative) == baseline_aa:\n",
        "#                     s = tentative\n",
        "#                     baseline_aa = _aa(s)\n",
        "#                     edits.append((idx, codon, repl, \"lowG\"))\n",
        "#                     break\n",
        "\n",
        "#     return s, edits"
      ],
      "metadata": {
        "id": "xwmWytmEivsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enrich AT-rich codons at N terminus\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xk0y2sRVQW2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.science.org/doi/10.1126/science.1241934\n",
        "\n",
        " Most synthetic biology studies only investigate the first 10 codons of E. coli, neglecting to provide information on more downstream sequences, even though the ‚Äúramp sequence‚Äù has been demonstrated to encompass as many as 30‚Äì50 codons (Tuller and Zur, 2015). A large-scale comprehensive data analysis of recombinant proteins in E. coli also indicated that the impact of codon usage on protein expression is more significant than the influence of mRNA folding factors(Bo√´l et al., 2016). Recent studies on the ramp sequence suggest that codons 3‚Äì5 may act as gates controlling mRNA translation efficiency. The expression of different proteins may not be attributed to alterations in initiation but rather the elongation of codons 4 and 5. The probability of translational arrest during synthesis of the N-terminal pentapeptide may determine the overall efficiency of protein synthesis(Verma et al., 2019; Moreira et al., 2019).\n",
        "\n",
        "Reference: https://academic.oup.com/nar/article/51/5/2363/7016452?login=false\n",
        "\n",
        " Protein production levels and the full coding sequences were determined for 1459 gene variants in Escherichia coli. Using different machine learning approaches, these data were used to reveal correlations between codon usage and protein production. Interestingly, protein production levels can be relatively accurately predicted (Pearson correlation of 0.762) by a Random Forest model that only relies on the sequence information of the first eight codons. In this region, close to the translation initiation site, mRNA secondary structure rather than Codon Adaptation Index (CAI) is the key determinant of protein production. This study clearly demonstrates the key role of codons at the start of the coding sequence. Furthermore, these results imply that commonly used CAI-based codon optimization of the full coding sequence is not a very effective strategy. One should rather focus on optimizing protein production via reducing mRNA secondary structure formation with the first few codons.\n",
        "....\n",
        " Remarkably, we show that only a window covering codons 2‚Äì8 is required to accurately predict mRFP production, based on sequence information only. This further strengthens the conclusions from previous studies and demonstrates that other codons later in the CDS in this study are not important to explain protein production. This also underlines that future studies aiming to optimize protein production should focus mostly on the codon usage of the 5‚Ä≤ start of the coding sequence, rather than the current practice of full CDS optimization.\n",
        "\n",
        "...\n",
        "\n",
        "The bases 6‚Äì25 correspond to codons 2‚Äì8. The nucleotides in the third position of these codons that contribute to high expression are mostly A, as well as T in codon 2\n",
        "\n",
        " Kudla G., Murray A.W., Tollervey D., Plotkin J.B. Coding-sequence determinants of gene expression in Escherichia coli. Science. 2009; 324:255‚Äì258.\n",
        " Cambray G., Guimaraes J.C., Arkin A.P. Evaluation of 244,000 synthetic sequences reveals design principles to optimize translation in Escherichia coli. Nat. Biotechnol. 2018; 36:1005‚Äì1015.\n",
        " Welch M., Govindarajan S., Ness J.E., Villalobos A., Gurney A., Minshull J., Gustafsson C. Design parameters to control synthetic gene expression in Escherichia coli. PLoS One. 2009; 4:e7002.\n",
        "\n",
        " Reference: https://www.jbc.org/article/S0021-9258(23)00318-6/fulltext\n",
        "\n",
        " The insertion of the DNA sequence encoding SKIK peptide adjacent to the M start codon of a difficult-to-express protein enhances protein production in Escherichia coli. In this report, we reveal that the increased production of the SKIK-tagged protein is not due to codon usage of the SKIK sequence.\n",
        "\n",
        " We reported previously that the insertion of codons 5‚Ä≤-TCT AAA ATA AAA-3‚Ä≤ or synonymous codons 5‚Ä≤-TCG AAG ATC AAG-3‚Ä≤ that encode four amino acids, Ser-Lys-Ile-Lys (SKIK), immediately after the Met start codon markedly increased the production level of proteins that were difficult to express in Escherichia coli\n",
        "\n",
        " Reference: https://www.biorxiv.org/content/10.1101/2024.03.21.586065v1.abstract\n",
        "\n",
        " Protein synthesis efficiency is highly dependent on mRNA coding sequence. Furthermore, there is extensive evidence of a correlation between mRNA stability and protein expression level, though the mechanistic determinants remain unclear. Using yellow fluorescent protein (YFP) as a reporter gene, we herein demonstrate that adenosine (A) abundance in the first six codons is a critical determinant for achieving high protein synthesis in E. coli. Increasing A and/or decreasing guanosine (G) content in this region results in substantial increases in protein expression level both in vivo and in vitro that are correlated with steady-state mRNA concentration in vivo, and this effect is attributable to changes in the stability of the mRNA that are directly coupled to its translation efficiency. Increasing A content promotes mRNA incorporation into the functional 70S ribosomal initiation complex without altering its affinity for the 30S ribosomal subunit. These results support a model in which base composition in the first six codons modulates local mRNA folding energy to control the balance between productive translation initiation versus degradation of mRNAs bound to the 30S ribosomal subunit. Based on these findings, we developed a short N-terminal coding sequence that optimizes translation initiation efficiency for protein production in E. coli.\n",
        "\n",
        " Translation boosting sequence is SKIVKI but don't cite the SKIK group!"
      ],
      "metadata": {
        "id": "k8PuU_jQanNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= Codon utilities =========================\n",
        "BACTERIAL_TABLE = CodonTable.unambiguous_dna_by_id[11]\n",
        "CODON_TO_AA = BACTERIAL_TABLE.forward_table.copy()\n",
        "STOP_CODONS = set(BACTERIAL_TABLE.stop_codons)\n",
        "AA_TO_CODONS: Dict[str, List[str]] = {}\n",
        "for codon, aa in CODON_TO_AA.items():\n",
        "    AA_TO_CODONS.setdefault(aa, []).append(codon)\n",
        "\n",
        "# Sequences start AFTER ATG; edit codons 1..7 of provided insert\n",
        "WOBBLE_WINDOW = (1, 7)\n",
        "WOBBLE_FIRST_ALLOWED = {\"A\", \"T\"}  # codon 1 wobble\n",
        "WOBBLE_OTHERS_ALLOWED = {\"A\"}      # codons 2..7 wobble\n",
        "LOW_G_WINDOW = WOBBLE_WINDOW\n",
        "\n",
        "def _hamming(a: str, b: str) -> int:\n",
        "    return sum(x != y for x, y in zip(a, b))\n",
        "\n",
        "def _ordered_wobble_candidates(aa: str, codon: str, allowed: Set[str]) -> List[str]:\n",
        "    if aa not in AA_TO_CODONS:\n",
        "        return []\n",
        "    first2 = codon[:2]\n",
        "    cands = []\n",
        "    for wob in sorted(allowed):\n",
        "        cand = (first2 + wob).upper()\n",
        "        if cand in AA_TO_CODONS[aa]:\n",
        "            cands.append(cand)\n",
        "    others = [c for c in AA_TO_CODONS[aa] if c[-1] in allowed and c not in cands]\n",
        "    others.sort(key=lambda c: (_hamming(c, codon), c))\n",
        "    cands.extend(others)\n",
        "    return cands\n",
        "\n",
        "def _ordered_lowG_candidates(aa: str, codon: str, allowed: Optional[Set[str]]) -> List[str]:\n",
        "    if aa not in AA_TO_CODONS:\n",
        "        return []\n",
        "    pool = AA_TO_CODONS[aa]\n",
        "    if allowed is not None:\n",
        "        pool = [c for c in pool if c[-1] in allowed]\n",
        "    pool = [c for c in pool if c.count(\"G\") < codon.count(\"G\")]\n",
        "    first2 = codon[:2]\n",
        "    pool.sort(key=lambda c: (c.count(\"G\"), 0 if c[:2] == first2 else 1, _hamming(c, codon), c))\n",
        "    return pool\n",
        "\n",
        "def _passes_constraints_whole(seq_str: str) -> bool:\n",
        "    p = DnaOptimizationProblem(sequence=seq_str, constraints=build_constraints())\n",
        "    return p.all_constraints_pass()\n",
        "\n",
        "def _aa(seq: str) -> str:\n",
        "    return str(Seq(seq).translate(table=11))\n",
        "\n",
        "def wobble_and_reduce(seq: str) -> Tuple[str, List[Tuple[int, str, str, str]]]:\n",
        "    s = str(seq).upper()\n",
        "    edits: List[Tuple[int, str, str, str]] = []\n",
        "    if len(s) < 3 or len(s) % 3 != 0:\n",
        "        return s, edits\n",
        "\n",
        "    baseline_aa = _aa(s)  # cache AA\n",
        "    n_codons = len(s) // 3\n",
        "\n",
        "    # Pass 1: wobble\n",
        "    w_start, w_end = WOBBLE_WINDOW\n",
        "    w_end = min(w_end, n_codons)\n",
        "    if w_start <= w_end:\n",
        "        for idx in range(w_start, w_end + 1):\n",
        "            pos = (idx - 1) * 3\n",
        "            codon = s[pos:pos+3]\n",
        "            if codon in STOP_CODONS:\n",
        "                break\n",
        "            aa = CODON_TO_AA.get(codon)\n",
        "            if not aa:\n",
        "                continue\n",
        "            allowed = WOBBLE_FIRST_ALLOWED if idx == w_start else WOBBLE_OTHERS_ALLOWED\n",
        "            if codon[-1] in allowed:\n",
        "                continue\n",
        "            for repl in _ordered_wobble_candidates(aa, codon, allowed):\n",
        "                tentative = s[:pos] + repl + s[pos+3:]\n",
        "                if _passes_constraints_whole(tentative) and _aa(tentative) == baseline_aa:\n",
        "                    s = tentative\n",
        "                    baseline_aa = _aa(s)\n",
        "                    edits.append((idx, codon, repl, \"wobble\"))\n",
        "                    break\n",
        "\n",
        "    # Pass 2: low-G within window\n",
        "    lg_start, lg_end = LOW_G_WINDOW\n",
        "    lg_end = min(lg_end, n_codons)\n",
        "    if lg_start <= lg_end:\n",
        "        for idx in range(lg_start, lg_end + 1):\n",
        "            pos = (idx - 1) * 3\n",
        "            codon = s[pos:pos+3]\n",
        "            if codon in STOP_CODONS:\n",
        "                break\n",
        "            aa = CODON_TO_AA.get(codon)\n",
        "            if not aa:\n",
        "                continue\n",
        "            allowed = WOBBLE_FIRST_ALLOWED if idx == w_start else WOBBLE_OTHERS_ALLOWED\n",
        "            for repl in _ordered_lowG_candidates(aa, codon, allowed):\n",
        "                tentative = s[:pos] + repl + s[pos+3:]\n",
        "                if _passes_constraints_whole(tentative) and _aa(tentative) == baseline_aa:\n",
        "                    s = tentative\n",
        "                    baseline_aa = _aa(s)\n",
        "                    edits.append((idx, codon, repl, \"lowG\"))\n",
        "                    break\n",
        "\n",
        "    return s, edits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4k5rxiOUXl2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DNA Chisel check after codon optimisation and wobble inserts"
      ],
      "metadata": {
        "id": "l75FocQVa2Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= Insert-only DNA Chisel check =========================\n",
        "def build_insert_only_constraints(insert_start: int, insert_end: int):\n",
        "    loc = (insert_start, insert_end)\n",
        "    return [\n",
        "        AvoidPattern(\"BsaI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"SapI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"BbsI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"BsmBI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"EcoRI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"NotI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"XbaI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"SpeI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"PstI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"BtgZI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"AarI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"NheI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"XhoI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"BamHI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"BglII_site\", strand='both', location=loc),\n",
        "        AvoidPattern(\"NruI_site\", strand='both', location=loc),\n",
        "        AvoidPattern(HomopolymerPattern(\"A\", 6), location=loc),\n",
        "        AvoidPattern(HomopolymerPattern(\"T\", 6), location=loc),\n",
        "        AvoidPattern(HomopolymerPattern(\"C\", 6), location=loc),\n",
        "        AvoidPattern(HomopolymerPattern(\"G\", 6), location=loc),\n",
        "        EnforceGCContent(mini=0.4, maxi=0.6, window=100, location=loc),\n",
        "        EnforceTranslation(genetic_table='Bacterial', location=loc),\n",
        "    ]\n",
        "\n",
        "def check_insert_region(final_construct: str, insert_start: int, insert_end: int):\n",
        "    p = DnaOptimizationProblem(\n",
        "        sequence=final_construct,\n",
        "        constraints=build_insert_only_constraints(insert_start, insert_end),\n",
        "        objectives=[]\n",
        "    )\n",
        "    return p.all_constraints_pass(), p.constraints_text_summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd9mbqjaazDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add flanking regions for Type IIS Assembly"
      ],
      "metadata": {
        "id": "y3rh8GWxbCz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= Builders (simplified) =========================\n",
        "def make_popenv3_only(\n",
        "    insert_seq: str,\n",
        "    *,\n",
        "    flank5: str = DEFAULT_FLANK5, flank3: str = DEFAULT_FLANK3,\n",
        "    nn_bbs: str = DEFAULT_BBSI_NN,\n",
        "):\n",
        "    insert = str(insert_seq).upper()\n",
        "    left  = (flank5 + BBSI_SITE_FW + nn_bbs + BBSI_LEFT_OVERHANG).upper()\n",
        "    right = (BBSI_RIGHT_OVERHANG + nn_bbs + BBSI_SITE_RV + flank3).upper()\n",
        "    construct = left + insert + right\n",
        "    meta = {\n",
        "        \"insert_start\": len(left), \"insert_end\": len(left) + len(insert),\n",
        "        \"flank5\": flank5.upper(), \"flank3\": flank3.upper(), \"nn_bbs\": nn_bbs.upper(),\n",
        "    }\n",
        "    return construct, meta\n",
        "\n",
        "def make_cd_part_with_bsaI(\n",
        "    insert_seq: str,\n",
        "    *,\n",
        "    left_bsa_letter: str = \"C\", right_bsa_letter: str = \"D\",\n",
        "    flank5: str = DEFAULT_FLANK5, flank3: str = DEFAULT_FLANK3,\n",
        "    nn_bbs: str = DEFAULT_BBSI_NN,\n",
        "    spacer_between_sites: str = DEFAULT_SPACER_BETWEEN_SITES,\n",
        "    spacer_right: str = DEFAULT_SPACER_RIGHT,\n",
        "    n_bsa: str = DEFAULT_BSAI_N,\n",
        "):\n",
        "    \"\"\"\n",
        "    Reclone CD part (top strand):\n",
        "      flank5 BBSI_SITE_FW nn_bbs CGCT spacer_between_sites\n",
        "      BSAI_SITE_FW n_bsa [LEFT5_PREFIX] LEFT_BSA [LEFT5_SUFFIX]  [INSERT]\n",
        "      [RIGHT3_PREFIX] RIGHT_BSA [RIGHT3_SUFFIX] n_bsa BSAI_SITE_RV spacer_right\n",
        "      GGAG nn_bbs BBSI_SITE_RV flank3\n",
        "    \"\"\"\n",
        "    # Resolve position-aware extras\n",
        "    Lpre, Lbsa, Lsuf = resolve_bsa_end(left_bsa_letter,  side=\"5\")\n",
        "    Rpre, Rbsa, Rsuf = resolve_bsa_end(right_bsa_letter, side=\"3\")\n",
        "\n",
        "    insert = str(insert_seq).upper()\n",
        "\n",
        "    # LEFT segment up to the insert start (includes any 5‚Ä≤ prefix/suffix around the left overhang)\n",
        "    left = (\n",
        "        flank5 + BBSI_SITE_FW + nn_bbs + BBSI_LEFT_OVERHANG +\n",
        "        spacer_between_sites + BSAI_SITE_FW + n_bsa +\n",
        "        Lpre + Lbsa + Lsuf\n",
        "    ).upper()\n",
        "\n",
        "    # RIGHT segment after the insert end (may include a 3‚Ä≤ prefix BEFORE the right overhang and a suffix AFTER it)\n",
        "    right = (\n",
        "        Rpre + Rbsa + Rsuf + n_bsa + BSAI_SITE_RV +\n",
        "        spacer_right + BBSI_RIGHT_OVERHANG + nn_bbs + BBSI_SITE_RV + flank3\n",
        "    ).upper()\n",
        "\n",
        "    construct = left + insert + right\n",
        "    meta = {\n",
        "        \"insert_start\": len(left), \"insert_end\": len(left) + len(insert),\n",
        "        \"flank5\": flank5.upper(), \"flank3\": flank3.upper(), \"nn_bbs\": nn_bbs.upper(),\n",
        "        \"spacer_between_sites\": spacer_between_sites.upper(), \"spacer_right\": spacer_right.upper(),\n",
        "        \"n_bsa\": n_bsa.upper(),\n",
        "        \"LEFT_BSA_PRE\": Lpre, \"LEFT_BSA\": Lbsa, \"LEFT_BSA_SUF\": Lsuf,\n",
        "        \"RIGHT_BSA_PRE\": Rpre, \"RIGHT_BSA\": Rbsa, \"RIGHT_BSA_SUF\": Rsuf,\n",
        "    }\n",
        "    return construct, meta\n",
        "\n",
        "def build_final_part(insert_seq: str, mode=\"cd\", **kwargs):\n",
        "    if mode == \"cd\":\n",
        "        return make_cd_part_with_bsaI(insert_seq, **kwargs)\n",
        "    elif mode == \"popenv3\":\n",
        "        return make_popenv3_only(insert_seq, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'cd' or 'popenv3'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "q-J6IJwPbHce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardrail Checks"
      ],
      "metadata": {
        "id": "2Yb3ip_MbPIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _assert_slice(seqU: str, start: int, token: str, label: str) -> int:\n",
        "    if not token:\n",
        "        return start  # allow zero-length extras\n",
        "    end = start + len(token)\n",
        "    if seqU[start:end] != token:\n",
        "        got = seqU[start:end]\n",
        "        raise ValueError(f\"Guardrail fail at {label}: expected '{token}' at {start}:{end}, got '{got}'\")\n",
        "    return end\n",
        "\n",
        "def assert_cd_layout(final_seq: str, meta: dict) -> bool:\n",
        "    s = final_seq.upper(); i = 0\n",
        "    i = _assert_slice(s, i, meta[\"flank5\"], \"flank5\")\n",
        "    i = _assert_slice(s, i, BBSI_SITE_FW, \"BbsI_fw_site\")\n",
        "    i = _assert_slice(s, i, meta[\"nn_bbs\"], \"BbsI_NN\")\n",
        "    i = _assert_slice(s, i, BBSI_LEFT_OVERHANG, \"Left_Bbs_overhang\")\n",
        "    i = _assert_slice(s, i, meta[\"spacer_between_sites\"], \"spacer_between_sites\")\n",
        "    i = _assert_slice(s, i, BSAI_SITE_FW, \"BsaI_fw_site\")\n",
        "    i = _assert_slice(s, i, meta[\"n_bsa\"], \"BsaI_N\")\n",
        "\n",
        "    # 5‚Ä≤ extras before/after left BsaI overhang\n",
        "    i = _assert_slice(s, i, meta[\"LEFT_BSA_PRE\"], \"Left_Bsa_prefix\")\n",
        "    i = _assert_slice(s, i, meta[\"LEFT_BSA\"], \"Left_Bsa_overhang\")\n",
        "    i = _assert_slice(s, i, meta[\"LEFT_BSA_SUF\"], \"Left_Bsa_suffix\")\n",
        "\n",
        "    # Insert starts here\n",
        "    if i != meta[\"insert_start\"]:\n",
        "        raise ValueError(f\"Guardrail fail: insert_start mismatch. Expected {meta['insert_start']}, got {i}\")\n",
        "\n",
        "    # Skip the insert region\n",
        "    i = meta[\"insert_end\"]\n",
        "\n",
        "    # 3‚Ä≤ extras: optional prefix BEFORE right overhang, suffix AFTER\n",
        "    i = _assert_slice(s, i, meta[\"RIGHT_BSA_PRE\"], \"Right_Bsa_prefix\")\n",
        "    i = _assert_slice(s, i, meta[\"RIGHT_BSA\"], \"Right_Bsa_overhang\")\n",
        "    i = _assert_slice(s, i, meta[\"RIGHT_BSA_SUF\"], \"Right_Bsa_suffix\")\n",
        "\n",
        "    i = _assert_slice(s, i, meta[\"n_bsa\"], \"BsaI_N_2\")\n",
        "    i = _assert_slice(s, i, BSAI_SITE_RV, \"BsaI_rev_site\")\n",
        "    i = _assert_slice(s, i, meta[\"spacer_right\"], \"spacer_right\")\n",
        "    i = _assert_slice(s, i, BBSI_RIGHT_OVERHANG, \"Right_Bbs_overhang\")\n",
        "    i = _assert_slice(s, i, meta[\"nn_bbs\"], \"BbsI_NN_2\")\n",
        "    i = _assert_slice(s, i, BBSI_SITE_RV, \"BbsI_rev_site_2\")\n",
        "    i = _assert_slice(s, i, meta[\"flank3\"], \"flank3\")\n",
        "\n",
        "    if i != len(s):\n",
        "        raise ValueError(\"Guardrail fail: trailing sequence after expected end.\")\n",
        "    return True\n",
        "\n"
      ],
      "metadata": {
        "id": "qGzqtHpabQld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and print outputs"
      ],
      "metadata": {
        "id": "6RqOP-MKbVU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= Verbose per-record processing + outputs =========================\n",
        "# CONFIG you may tweak\n",
        "BUILD_MODE = \"cd\"          # \"cd\" or \"popenv3\"\n",
        "# Use tags from CSV if available, otherwise fall back to these defaults\n",
        "DEFAULT_LEFT_BSA_LETTER  = \"C\"\n",
        "DEFAULT_RIGHT_BSA_LETTER = \"D\"\n",
        "\n",
        "# Optional: linkers/flanks (keep defaults unless you need to change)\n",
        "LINKER_BETWEEN_SITES = DEFAULT_SPACER_BETWEEN_SITES\n",
        "LINKER_RIGHT         = DEFAULT_SPACER_RIGHT\n",
        "BBSI_NN              = DEFAULT_BBSI_NN\n",
        "BSAI_N               = DEFAULT_BSAI_N\n",
        "FLANK5               = DEFAULT_FLANK5\n",
        "FLANK3               = DEFAULT_FLANK3\n",
        "\n",
        "# Ensure we have places to put results\n",
        "optimized_sequences: List[SeqRecord] = [] # Enriched inserts (no adapters)\n",
        "final_sequences: List[str] = [] # Raw sequence strings of enriched inserts\n",
        "final_records: List[SeqRecord] = [] # SeqRecords of enriched inserts (redundant with optimized_sequences?)\n",
        "synth_records: List[SeqRecord] = [] # Synthesis-ready parts (with adapters)\n",
        "\n",
        "print(f\"\\n=== Building parts | mode={BUILD_MODE}\"\n",
        "      + (f\" | Default BsaI {DEFAULT_LEFT_BSA_LETTER}->{DEFAULT_RIGHT_BSA_LETTER}\" if BUILD_MODE=='cd' else \"\")\n",
        "      + f\" | n={len(dna_records) if 'dna_records' in locals() else 0} ===\") # Check if dna_records is defined\n",
        "\n",
        "# Ensure dna_records is defined and not empty before proceeding\n",
        "if 'dna_records' not in locals() or not dna_records:\n",
        "    print(\"üö´ dna_records list is empty or not defined. Please run the previous step (reverse translation) first.\")\n",
        "else:\n",
        "    for i, rec in enumerate(dna_records):\n",
        "        rec_name = getattr(rec, \"name\", None) or getattr(rec, \"id\", None) or f\"insert_{i+1}\"\n",
        "        initial_dna_seq = str(rec.seq) # Get the initial DNA from the dna_records list\n",
        "\n",
        "        print(f\"\\n[{i+1}/{len(dna_records)}] Processing {rec_name}\")\n",
        "        print(f\"  ‚Ä¢ Starting with initial DNA: len={len(initial_dna_seq)} bp | GC={_gc_pct(initial_dna_seq):.1f}% | G={initial_dna_seq.count('G')}\")\n",
        "\n",
        "        # 1) Optimize with DNA Chisel (starting from the initial DNA)\n",
        "        # Note: If you started with protein in the previous step, this DNA is\n",
        "        # already a basic reverse translation. This step optimizes it further.\n",
        "        problem = DnaOptimizationProblem(\n",
        "            sequence=initial_dna_seq, # Start with the initial DNA sequence\n",
        "            constraints=build_constraints(), # Assumes build_constraints is defined\n",
        "            objectives=[CodonOptimize(species='e_coli')] # Optimize codons\n",
        "        )\n",
        "        print(\"  ‚Ä¢ Starting DNA Chisel optimization...\")\n",
        "        try:\n",
        "            problem.resolve_constraints()\n",
        "            problem.optimize()\n",
        "            final_optimized_dna = problem.sequence\n",
        "            print(f\"  ‚Ä¢ DNA Chisel optimized: len={len(final_optimized_dna)} bp | GC={_gc_pct(final_optimized_dna):.1f}% | G={final_optimized_dna.count('G')}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error during DNA Chisel optimization for {rec_name}: {e}\")\n",
        "            final_optimized_dna = initial_dna_seq # Revert to initial DNA if optimization fails\n",
        "            print(\"    Using initial DNA sequence for subsequent steps.\")\n",
        "\n",
        "\n",
        "        # 2) Wobble/lowG (first 7 codons only) - applied to the optimized DNA\n",
        "        enriched_seq, edits = wobble_and_reduce(final_optimized_dna) # Assumes wobble_and_reduce is defined\n",
        "        print(f\"  ‚Ä¢ wobble/lowG edits: n={len(edits)} | ŒîG={enriched_seq.count('G') - final_optimized_dna.count('G')}\"\n",
        "              f\" | GC { _gc_pct(final_optimized_dna):.1f}% ‚Üí { _gc_pct(enriched_seq):.1f}%\")\n",
        "        print(f\"    edits: {_short_edits(edits)}\")\n",
        "\n",
        "        # AA safety check (compare translation of optimized DNA vs enriched DNA)\n",
        "        try:\n",
        "            aa_ok = (_aa(final_optimized_dna).rstrip(\"*\") == _aa(enriched_seq).rstrip(\"*\")) # Assumes _aa is defined\n",
        "            print(f\"  ‚Ä¢ AA unchanged after edits? {'YES' if aa_ok else 'NO (unexpected!)'}\")\n",
        "            if not aa_ok:\n",
        "                print(\"    ‚ö†Ô∏è Reverting to DNA Chisel output to preserve translation.\")\n",
        "                enriched_seq = final_optimized_dna # Revert to DNA from CodonOptimize if AA changes\n",
        "                edits = []\n",
        "        except Exception as e:\n",
        "             print(f\"  ‚ö†Ô∏è Warning: Could not verify AA unchanged after edits: {e}. Proceeding with enriched sequence.\")\n",
        "             aa_ok = False # Cannot confirm, mark as potentially not OK\n",
        "\n",
        "        # Whole-insert constraint check (on the enriched sequence)\n",
        "        check_problem_enriched = DnaOptimizationProblem(\n",
        "            sequence=enriched_seq,\n",
        "            constraints=build_constraints() # Assumes build_constraints is defined\n",
        "        )\n",
        "        constraints_ok_enriched = check_problem_enriched.all_constraints_pass()\n",
        "        if constraints_ok_enriched:\n",
        "            print(f\"  ‚Ä¢ Constraints after edits: ‚úÖ PASS\")\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ Constraints after edits: ‚ùå FAIL ‚Äî details below\")\n",
        "            print(check_problem_enriched.constraints_text_summary())\n",
        "\n",
        "\n",
        "        # Save enriched insert (no adapters yet)\n",
        "        optimized_sequences.append(\n",
        "            SeqRecord(Seq(enriched_seq), id=rec_name,\n",
        "                      description=f\"Optimized + wobble/lowG | edits={len(edits)} | \"\n",
        "                                  f\"AA={'OK' if aa_ok else 'MISMATCH'} | constraints={'OK' if constraints_ok_enriched else 'FAIL'}\")\n",
        "        )\n",
        "        final_sequences.append(enriched_seq)\n",
        "        # Add the enriched sequence as a record for final_records (used by problem.to_record potentially, but simpler to just store the seq)\n",
        "        # If you need the full DnaOptimizationProblem results, you'd save 'problem' or its summary here.\n",
        "        final_records.append(SeqRecord(Seq(enriched_seq), id=rec_name + \"_enriched\", description=\"Enriched sequence\"))\n",
        "\n",
        "\n",
        "        # 3) Build adapters + guardrails + insert-only check\n",
        "        # Use tags from the original row in 'parts' if available, fallback to defaults\n",
        "        # Need to find the corresponding original row in 'parts'\n",
        "        original_row = next((item for item in parts if item[\"name\"] == rec_name), None)\n",
        "        if original_row:\n",
        "             current_left_tag = original_row.get(\"left_tag\", DEFAULT_LEFT_BSA_LETTER)\n",
        "             current_right_tag = original_row.get(\"right_tag\", DEFAULT_RIGHT_BSA_LETTER)\n",
        "             print(f\"  ‚Ä¢ Using tags from CSV: {current_left_tag}-{current_right_tag}\")\n",
        "        else:\n",
        "             current_left_tag = DEFAULT_LEFT_BSA_LETTER\n",
        "             current_right_tag = DEFAULT_RIGHT_BSA_LETTER\n",
        "             print(f\"  ‚Ä¢ Original row not found for {rec_name}. Using default tags: {current_left_tag}-{current_right_tag}\")\n",
        "\n",
        "\n",
        "        if BUILD_MODE == \"cd\":\n",
        "            try:\n",
        "                final_construct, meta = build_final_part(\n",
        "                    enriched_seq, mode=\"cd\",\n",
        "                    left_bsa_letter=current_left_tag, right_bsa_letter=current_right_tag,\n",
        "                    flank5=FLANK5, flank3=FLANK3,\n",
        "                    nn_bbs=BBSI_NN, n_bsa=BSAI_N,\n",
        "                    spacer_between_sites=LINKER_BETWEEN_SITES, spacer_right=LINKER_RIGHT,\n",
        "                )\n",
        "                print(\"  ‚Ä¢ Built CD layout with adapters.\")\n",
        "                # Guardrails: exact motif placement\n",
        "                try:\n",
        "                    assert_cd_layout(final_construct, meta) # Assumes assert_cd_layout is defined\n",
        "                    print(\"    Guardrails: ‚úÖ CD layout OK\")\n",
        "                    guardrail_status = \"PASS\"\n",
        "                except Exception as e:\n",
        "                    print(f\"    Guardrails: ‚ùå CD layout FAILED ‚Äî {e}\")\n",
        "                    guardrail_status = f\"FAIL: {e}\"\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"  ‚ùå Error building CD part for {rec_name}: {e}\")\n",
        "                 final_construct = \"\"\n",
        "                 meta = {}\n",
        "                 guardrail_status = f\"BUILD ERROR: {e}\"\n",
        "\n",
        "        else: # BUILD_MODE == \"popenv3\"\n",
        "            try:\n",
        "                final_construct, meta = build_final_part(\n",
        "                    enriched_seq, mode=\"popenv3\",\n",
        "                    flank5=FLANK5, flank3=FLANK3,\n",
        "                    nn_bbs=BBSI_NN\n",
        "                )\n",
        "                print(\"  ‚Ä¢ Built pOpenv3 layout with adapters.\")\n",
        "                # Guardrails: exact motif placement\n",
        "                try:\n",
        "                    assert_popenv3_layout(final_construct, meta) # Assumes assert_popenv3_layout is defined\n",
        "                    print(\"    Guardrails: ‚úÖ pOpenv3 layout OK\")\n",
        "                    guardrail_status = \"PASS\"\n",
        "                except Exception as e:\n",
        "                    print(f\"    Guardrails: ‚ùå pOpenv3 layout FAILED ‚Äî {e}\")\n",
        "                    guardrail_status = f\"FAIL: {e}\"\n",
        "            except Exception as e:\n",
        "                 print(f\"  ‚ùå Error building pOpenv3 part for {rec_name}: {e}\")\n",
        "                 final_construct = \"\"\n",
        "                 meta = {}\n",
        "                 guardrail_status = f\"BUILD ERROR: {e}\"\n",
        "\n",
        "\n",
        "        # Insert-only DNA Chisel check (we ignore adapters on purpose)\n",
        "        if final_construct and meta: # Only check if construct was built\n",
        "            try:\n",
        "                ok_insert, summary = check_insert_region(final_construct, meta[\"insert_start\"], meta[\"insert_end\"]) # Assumes check_insert_region is defined\n",
        "                print(f\"  ‚Ä¢ Insert-only constraints after adapters: {'‚úÖ PASS' if ok_insert else '‚ùå FAIL'}\")\n",
        "                insert_check_status = 'OK' if ok_insert else 'FAIL'\n",
        "                if not ok_insert:\n",
        "                    print(\"    Details:\")\n",
        "                    print(summary)\n",
        "            except Exception as e:\n",
        "                 print(f\"  ‚ùå Error during insert-only check for {rec_name}: {e}\")\n",
        "                 insert_check_status = f\"CHECK ERROR: {e}\"\n",
        "        else:\n",
        "            print(\"  ‚Ä¢ Skipping insert-only constraints check due to build errors.\")\n",
        "            insert_check_status = \"SKIPPED\"\n",
        "\n",
        "\n",
        "        # Save adapterized construct\n",
        "        if final_construct: # Only add if construct was built\n",
        "            mode_tag = \"CD\" if BUILD_MODE == \"cd\" else \"pOpenv3\"\n",
        "            extra_tag = (f\"({meta.get('LEFT_BSA', '?')}-{meta.get('RIGHT_BSA', '?')})\" if BUILD_MODE == \"cd\" else \"\")\n",
        "            synth_records.append(\n",
        "                SeqRecord(\n",
        "                    Seq(final_construct),\n",
        "                    id=f\"{rec_name}|{mode_tag}{extra_tag}\",\n",
        "                    description=f\"insert={meta.get('insert_start', '?')}..{meta.get('insert_end', '?')} | insert-check={insert_check_status} | guardrails={guardrail_status}\"\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "             print(f\"  üö´ Skipping synthesis part record for {rec_name} due to build errors.\")\n",
        "\n",
        "\n",
        "# ======================= AFTER THE LOOP: write files =======================\n",
        "if optimized_sequences: # Only write if there were sequences processed\n",
        "    out1 = \"optimized_sequences_enriched_checked.fasta\"\n",
        "    out2 = \"synthesis_parts.fasta\"\n",
        "    try:\n",
        "        with open(out1, \"w\") as fh1:\n",
        "            SeqIO.write(optimized_sequences, fh1, \"fasta\")\n",
        "        print(f\"\\n‚úÖ Done writing FASTA outputs.\")\n",
        "        print(f\"   ‚Ä¢ Enriched inserts (no adapters): {out1}  (n={len(optimized_sequences)})\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error writing {out1}: {e}\")\n",
        "\n",
        "    try:\n",
        "        with open(out2, \"w\") as fh2:\n",
        "            SeqIO.write(synth_records, fh2, \"fasta\")\n",
        "        print(f\"   ‚Ä¢ Synthesis-ready parts ({mode_tag}): {out2}  (n={len(synth_records)})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error writing {out2}: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nüö´ No sequences were processed successfully to generate outputs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnW-6jOabXZc",
        "outputId": "7db31506-a392-4126-903f-f300794e8e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Building parts | mode=cd | Default BsaI C->D | n=3 ===\n",
            "\n",
            "[1/3] Processing mVirD2\n",
            "  ‚Ä¢ Starting with initial DNA: len=27 bp | GC=40.7% | G=4\n",
            "  ‚Ä¢ Starting DNA Chisel optimization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "objective:   0%|          | 0/1 [00:00<?, ?it/s, now=MaximizeCAI[0-27](e_coli)...]\n",
            "location:   0%|          | 0/6 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "location:   0%|          | 0/6 [00:00<?, ?it/s, now=0-3] \u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚Ä¢ DNA Chisel optimized: len=27 bp | GC=63.0% | G=8\n",
            "  ‚Ä¢ wobble/lowG edits: n=6 | ŒîG=-3 | GC 63.0% ‚Üí 44.4%\n",
            "    edits: 1:AGC>AGT(wobble), 3:CAG>CAA(wobble), 4:CCG>CCA(wobble), 5:ACC>ACA(wobble), 6:CGC>CGA(wobble), 1:AGT>TCT(lowG)\n",
            "  ‚Ä¢ AA unchanged after edits? YES\n",
            "  ‚Ä¢ Constraints after edits: ‚úÖ PASS\n",
            "  ‚Ä¢ Using tags from CSV: C-D\n",
            "  ‚Ä¢ Built CD layout with adapters.\n",
            "    Guardrails: ‚úÖ CD layout OK\n",
            "  ‚Ä¢ Insert-only constraints after adapters: ‚úÖ PASS\n",
            "\n",
            "[2/3] Processing sfGFP\n",
            "  ‚Ä¢ Starting with initial DNA: len=39 bp | GC=33.3% | G=9\n",
            "  ‚Ä¢ Starting DNA Chisel optimization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "objective:   0%|          | 0/1 [00:00<?, ?it/s, now=MaximizeCAI[0-39](e_coli)...]\n",
            "location:   0%|          | 0/8 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "location:   0%|          | 0/8 [00:00<?, ?it/s, now=3-6] \u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚Ä¢ DNA Chisel optimized: len=39 bp | GC=56.4% | G=14\n",
            "  ‚Ä¢ wobble/lowG edits: n=3 | ŒîG=-2 | GC 56.4% ‚Üí 48.7%\n",
            "    edits: 2:AGC>TCA(wobble), 4:GGC>GGA(wobble), 7:CTG>CTA(wobble)\n",
            "  ‚Ä¢ AA unchanged after edits? YES\n",
            "  ‚Ä¢ Constraints after edits: ‚úÖ PASS\n",
            "  ‚Ä¢ Using tags from CSV: B-C\n",
            "  ‚Ä¢ Built CD layout with adapters.\n",
            "    Guardrails: ‚úÖ CD layout OK\n",
            "  ‚Ä¢ Insert-only constraints after adapters: ‚úÖ PASS\n",
            "\n",
            "[3/3] Processing tagRFP\n",
            "  ‚Ä¢ Starting with initial DNA: len=36 bp | GC=41.7% | G=6\n",
            "  ‚Ä¢ Starting DNA Chisel optimization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "objective:   0%|          | 0/1 [00:00<?, ?it/s, now=MaximizeCAI[0-36](e_coli)...]\n",
            "location:   0%|          | 0/6 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "location:   0%|          | 0/6 [00:00<?, ?it/s, now=0-3] \u001b[A\n",
            "                                                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚Ä¢ DNA Chisel optimized: len=36 bp | GC=58.3% | G=8\n",
            "  ‚Ä¢ wobble/lowG edits: n=3 | ŒîG=-1 | GC 58.3% ‚Üí 50.0%\n",
            "    edits: 1:ACC>ACA(wobble), 2:GGC>GGA(wobble), 3:AGC>TCA(wobble)\n",
            "  ‚Ä¢ AA unchanged after edits? YES\n",
            "  ‚Ä¢ Constraints after edits: ‚úÖ PASS\n",
            "  ‚Ä¢ Using tags from CSV: N1-N5\n",
            "  ‚Ä¢ Built CD layout with adapters.\n",
            "    Guardrails: ‚úÖ CD layout OK\n",
            "  ‚Ä¢ Insert-only constraints after adapters: ‚úÖ PASS\n",
            "\n",
            "‚úÖ Done writing FASTA outputs.\n",
            "   ‚Ä¢ Enriched inserts (no adapters): optimized_sequences_enriched_checked.fasta  (n=3)\n",
            "   ‚Ä¢ Synthesis-ready parts (CD): synthesis_parts.fasta  (n=3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kt1bgULwafz7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfa9a0b5",
        "outputId": "fca1a658-52c7-41f9-833a-cd7ac99858d0"
      },
      "source": [
        "# ======================= Generate updated CSV =======================\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure 'parts' (from the input CSV) and 'synth_records' (from the processing cell) are defined\n",
        "\n",
        "# Create a list of dictionaries to hold the combined data\n",
        "combined_data = []\n",
        "# Create a dictionary mapping synth_record ID (part name part) to sequence for easy lookup\n",
        "# Split the ID to get the base part name before the \"|\"\n",
        "synth_seq_map = {rec.id.split('|')[0]: str(rec.seq) for rec in synth_records}\n",
        "\n",
        "# Assuming 'parts' list is available from the CSV loading step\n",
        "# If 'parts' is not defined here, you might need to add a check or reload it.\n",
        "try:\n",
        "    parts\n",
        "except NameError:\n",
        "    print(\"Error: 'parts' data not found. Please run the CSV upload cell first.\")\n",
        "    parts = [] # Initialize as empty to avoid further errors\n",
        "\n",
        "if parts:\n",
        "    for row in parts:\n",
        "        row_name = row.get(\"name\")\n",
        "        if row_name in synth_seq_map:\n",
        "            # Create a new dictionary for the row, copying original data\n",
        "            new_row = row.copy()\n",
        "            # Add the synthesis sequence\n",
        "            new_row[\"synthesis_seq\"] = synth_seq_map[row_name]\n",
        "            combined_data.append(new_row)\n",
        "        else:\n",
        "             # If a synth record wasn't generated (e.g., due to errors), still include the original row\n",
        "             # and indicate no synthesis sequence was generated.\n",
        "             new_row = row.copy()\n",
        "             new_row[\"synthesis_seq\"] = \"\" # Or a placeholder like \"Error/Skipped\"\n",
        "             combined_data.append(new_row)\n",
        "             print(f\"‚ö†Ô∏è Warning: No synthesis sequence generated for {row_name}. Excluding from CSV or marking as empty.\")\n",
        "\n",
        "\n",
        "    # Create a pandas DataFrame from the combined data\n",
        "    if combined_data:\n",
        "        try:\n",
        "            df_output = pd.DataFrame(combined_data)\n",
        "            output_csv_path = \"processed_parts_with_synthesis_seq.csv\"\n",
        "            df_output.to_csv(output_csv_path, index=False)\n",
        "            print(f\"\\n‚úÖ Generated updated CSV: {output_csv_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error generating or writing CSV: {e}\")\n",
        "    else:\n",
        "        print(\"\\nüö´ No combined data to write to CSV.\")\n",
        "else:\n",
        "    print(\"\\nüö´ 'parts' list is empty. Cannot generate updated CSV.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Generated updated CSV: processed_parts_with_synthesis_seq.csv\n"
          ]
        }
      ]
    }
  ]
}